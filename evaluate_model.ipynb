{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOodm+Dkf3pso6jKBRgn1c0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitrod/ehu_nlp_dimathina/blob/clean_branch/evaluate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "In this script you can evaluate any model that has been developed during the project using the evaluation data.\n",
        "\n",
        "By default the evaluation data has been set to be the entire validation split (7993 questions) of the triviaqa dataset.\n",
        "If you want to change the evaluation data to be a different split or a different default size for your tests you can do that using the *load_evaluation_data.py* script in the utils package. A guide for this is in the section [Changing the Evaluation Data](#scrollTo=H294GP72zhM9).\n",
        "\n"
      ],
      "metadata": {
        "id": "N6R7R18RhL1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xFRvUd8FjTnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "To import the model into the evaluation environment enter the parameters into the script below. Use the following table to find the right entries for each model:\n",
        "\n",
        "\n",
        "|Model|Context|MODEL_NAME|DATABASE|PARAMS|Note|\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "|Tiny Llama|-|tiny_llama_no_retriever|-|-|**very slow**|\n",
        "|Tiny Llama|Whole documents|tiny_llama_dense|**external**|-|**very slow**|\n",
        "|Mistral Instruct|-|mistral_instruct_no_retriever|-|-|**Huggingface Token and GPU required**|\n",
        "|Mistral Instruct|Whole documents|mistral_instruct_dense|**external**|-|**Huggingface Token and GPU required**|\n",
        "|Mistral Instruct|Text fragments|mistral_instruct_hybrid|sparse-dense|k, c, o|**Huggingface Token and GPU required**|\n",
        "|Bert Base|QA Pairs|bert_base_qa_embeddings|**directly imported**|k|-|\n",
        "|Bert Base|Whole documents|bert_base_dense|**external**|-|-|\n",
        "|Bert Base|Text fragments|bert_base_sparse|sparse|k|-|\n",
        "|Bert Finetuned|Whole documents|bert_finetuned_dense|**external**|-|-|\n",
        "|Chat GPT 4o|-|chat_gpt_no_retriever|-|t|**Not free to use**|\n",
        "|Chat GPT 4o|Text fragments|chat_gpt_hybrid|sparse-dense|k, c, o, t|**Not free to use**|\n",
        "\n",
        "The meaning of each parameter can be found in this table\n",
        "\n",
        "|Parameter Name|Description|\n",
        "|----------|----------|\n",
        "|k|Number of contexts the retriever sends to the reader|\n",
        "|c|chunk size of each context|\n",
        "|o|overlap between the contexts|\n",
        "|t|temparature of the reader model|\n",
        "\n",
        "If the model uses an external database, a directly imported database or no database please enter an empty string (\"\") for the DATABASE variable in the script.\n",
        "\n",
        "If the model doesn't have any parameter please enter \"-\" for the PARAMS variable. If the model requires several parameters separate each parameter by a blank space."
      ],
      "metadata": {
        "id": "iO2nNW6kjU1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH1mqdP9XZBY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import importlib\n",
        "from google.colab import files\n",
        "\n",
        "os.environ[\"MODEL_NAME\"] = \"bert_base_sparse\"\n",
        "os.environ[\"DATABASE\"] = \"sparse\"\n",
        "os.environ[\"PARAMS\"] = \"5\"\n",
        "os.environ[\"SPLIT_SIZE\"] = \"7993\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following script to setup the evaluation environment"
      ],
      "metadata": {
        "id": "LFeS1s_SYl3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Set environment variables\n",
        "directory = os.environ[\"MODEL_NAME\"]\n",
        "database = os.environ[\"DATABASE\"]\n",
        "\n",
        "# Install Git LFS\n",
        "!sudo apt-get install git-lfs -y\n",
        "!git lfs install\n",
        "\n",
        "# Clone the repositories\n",
        "!git clone https://github.com/mandarjoshi90/triviaqa.git\n",
        "!git clone --branch clean_branch https://github.com/dimitrod/ehu_nlp_dimathina.git\n",
        "%cd ehu_nlp_dimathina\n",
        "\n",
        "# Fetch and checkout requirements\n",
        "mod_path = f\"models/{directory}/*\"\n",
        "!git lfs fetch --include=\"{mod_path}, evaluation\"\n",
        "!git lfs checkout\n",
        "%cd ..\n",
        "\n",
        "# Move the model and the evaluation package to the current directory\n",
        "shutil.move(f\"ehu_nlp_dimathina/models/{directory}\", \".\")\n",
        "shutil.move(\"ehu_nlp_dimathina/evaluation\", \".\")\n",
        "\n",
        "#Create the results directory in the evaluation package\n",
        "!mkdir evaluation/results\n",
        "\n",
        "# Handle the optional database\n",
        "if database:\n",
        "    %cd ehu_nlp_dimathina\n",
        "    db_path = f\"databases/{database}/*\"\n",
        "    !git lfs fetch --include=\"{db_path}\"\n",
        "    !git lfs checkout\n",
        "    %cd ..\n",
        "    shutil.move(f\"ehu_nlp_dimathina/databases/{database}\", f\"{directory}/database\")\n",
        "\n",
        "\n",
        "# Install model-specific requirements\n",
        "!pip install -r {directory}/requirements.txt\n",
        "!pip install -r evaluation/requirements.txt\n",
        "!pip install -r triviaqa/requirements.txt\n",
        "\n",
        "# Cleanup\n",
        "shutil.rmtree(\"ehu_nlp_dimathina\")\n",
        "shutil.rmtree(\"sample_data\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3MJkoa5kiQ2t",
        "outputId": "818b4c4d-2bda-4cd7-f4ff-145c9c7053a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'triviaqa'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Total 70 (delta 0), reused 0 (delta 0), pack-reused 70 (from 1)\u001b[K\n",
            "Receiving objects: 100% (70/70), 20.60 KiB | 843.00 KiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Cloning into 'ehu_nlp_dimathina'...\n",
            "remote: Enumerating objects: 1922, done.\u001b[K\n",
            "remote: Counting objects: 100% (513/513), done.\u001b[K\n",
            "remote: Compressing objects: 100% (416/416), done.\u001b[K\n",
            "remote: Total 1922 (delta 275), reused 134 (delta 97), pack-reused 1409 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1922/1922), 30.10 MiB | 32.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1169/1169), done.\n",
            "/content/ehu_nlp_dimathina\n",
            "fetch: Fetching reference refs/heads/clean_branch\n",
            "Skipped checkout for \"databases/sparse-dense/document_library.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse-dense/documents.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse-dense/tfidf_vocabulary.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse/document_library.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse/documents.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse/tfidf_vocabulary.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_qa_embeddings/rag_qa_database/index.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_qa_embeddings/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_finetuned_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/chat_gpt_hybrid/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/chat_gpt_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_hybrid/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/tiny_llama_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/tiny_llama_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_qa_embeddings_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_sparse_embeddings_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_finetuned_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/chat_gpt_no_retriever_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/chat_gpt_sparse_dense_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/mistral_instruct_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/mistral_instruct_no_retriever_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/tiny_llama_no_retriever_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Checking out LFS objects: 100% (38/38), 1.5 GB | 0 B/s, done.\n",
            "/content\n",
            "/content/ehu_nlp_dimathina\n",
            "fetch: Fetching reference refs/heads/clean_branch\n",
            "Skipped checkout for \"databases/sparse-dense/document_library.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse-dense/documents.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"databases/sparse-dense/tfidf_vocabulary.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_qa_embeddings/rag_qa_database/index.pkl\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_base_qa_embeddings/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/bert_finetuned_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/chat_gpt_hybrid/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/chat_gpt_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_hybrid/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/mistral_instruct_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/tiny_llama_dense/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"models/tiny_llama_no_retriever/requirements.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_qa_embeddings_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_base_sparse_embeddings_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/bert_finetuned_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/chat_gpt_no_retriever_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/chat_gpt_sparse_dense_validation_split_size=7993_results.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/mistral_instruct_dense_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/mistral_instruct_no_retriever_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Skipped checkout for \"results/tiny_llama_no_retriever_validation_split_size=7993.txt\", content not local. Use fetch to download.\n",
            "Checking out LFS objects: 100% (38/38), 1.5 GB | 470 MB/s, done.\n",
            "/content\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r bert_base_sparse/requirements.txt (line 1)) (4.47.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from -r bert_base_sparse/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r bert_base_sparse/requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r bert_base_sparse/requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r bert_base_sparse/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r bert_base_sparse/requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->-r bert_base_sparse/requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->-r bert_base_sparse/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r bert_base_sparse/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r bert_base_sparse/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r bert_base_sparse/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r bert_base_sparse/requirements.txt (line 1)) (2024.12.14)\n",
            "Collecting datasets (from -r evaluation/requirements.txt (line 1))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r evaluation/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (4.67.1)\n",
            "Collecting xxhash (from datasets->-r evaluation/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r evaluation/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r evaluation/requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r evaluation/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r evaluation/requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets->-r evaluation/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r evaluation/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r evaluation/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r evaluation/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r evaluation/requirements.txt (line 1)) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r evaluation/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r evaluation/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r evaluation/requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r evaluation/requirements.txt (line 1)) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: tensorflow>=0.11 in /usr/local/lib/python3.10/dist-packages (from -r triviaqa/requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r triviaqa/requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r triviaqa/requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from -r triviaqa/requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r triviaqa/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r triviaqa/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r triviaqa/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->-r triviaqa/requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=0.11->-r triviaqa/requirements.txt (line 1)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CoPJQ6D3jpq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the Evaluation Data\n",
        "\n",
        "**If you don't want to change the split of the evaluation dataset skip this step**\n",
        "\n",
        "This step is only required if you want to evaluate with a different data set. You can change the evaluation dataset with the following command\n",
        "\n",
        "`!python3 -m Evaluation.utils.load_evaluation_data --split {split} --split_size {split_size}`\n",
        "\n",
        "Keep the maximum split sizes in mind"
      ],
      "metadata": {
        "id": "H294GP72zhM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m evaluation.utils.load_evaluation_data --split \"train\" --split_size 7900"
      ],
      "metadata": {
        "id": "cV7pwZT11Pik",
        "outputId": "58385919-7432-4b23-d967-b24d844aa09b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolving data files: 100% 26/26 [00:00<00:00, 145.27it/s]\n",
            "Loading dataset: 100% 7993/7993 [00:01<00:00, 4036.49it/s]\n",
            "Replacing field names: 100% 19/19 [00:00<00:00, 83.74it/s]\n",
            "Saving file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JPZ3SoyezfjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing the Evaluation Chain\n",
        "Now you can run the Evaluation chain with the model of choice. To start the evaluation chain use the following command\n",
        "\n",
        "`!python3 -m evaluation.evaluation_chain --model_name $MODEL_NAME --model_params $PARAMS --split_size $SPLIT_SIZE`"
      ],
      "metadata": {
        "id": "6uaV1IoKc2Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m evaluation.evaluation_chain --model_name $MODEL_NAME --model_params $PARAMS --split_size $SPLIT_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Uk8IFdc1qA",
        "outputId": "f6dde742-5c53-4d4a-e62f-57ce538ed3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rLoading Questions:   0% 0/7993 [00:00<?, ?it/s]\rLoading Questions: 100% 7993/7993 [00:00<00:00, 220735.40it/s]\n",
            "Loading model...\n",
            "2024-12-19 20:51:50.102172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-19 20:51:50.118045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-19 20:51:50.139078: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-19 20:51:50.145385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-19 20:51:50.160723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-19 20:51:51.371666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading vecorizer\n",
            "loading vector base\n",
            "loading documents\n",
            "loading model\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "config.json: 100% 473/473 [00:00<00:00, 3.44MB/s]\n",
            "model.safetensors: 100% 261M/261M [00:01<00:00, 222MB/s]\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 391kB/s]\n",
            "vocab.txt: 100% 213k/213k [00:00<00:00, 5.16MB/s]\n",
            "tokenizer.json: 100% 436k/436k [00:00<00:00, 21.7MB/s]\n",
            "Device set to use cuda:0\n",
            "Model initiated\n",
            "Starting QA...\n",
            "Retrieving Answers:   0% 10/7993 [00:14<3:03:24,  1.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Retrieving Answers:   4% 350/7993 [08:01<2:54:27,  1.37s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you automatically want to download the results of you evaluation Execute the following script (while the data is loading)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9nCQuGJNgqxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(f'evaluation/results/{os.environ[\"MODEL_NAME\"]}_split_size={os.environ[\"SPLIT_SIZE\"]}_results.txt')"
      ],
      "metadata": {
        "id": "nTaX2JR9rb2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}