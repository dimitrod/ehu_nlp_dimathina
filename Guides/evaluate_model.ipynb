{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5Ixd9M7y3awt8w1KY6SDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitrod/ehu_nlp_dimathina/blob/main/Guides/evaluate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "In this script you can evaluate any model that has been developed during the project using the evaluation data.\n",
        "\n",
        "By default the evaluation data has been set to be the first 7900 questions from the train split of the triviaqa dataset.\n",
        "If you want to change the evaluation data to be a different split you can do that using the *load_evaluation_data.py* script in the utils package. A guide for this is in the section [Changing the Evaluation Data](#scrollTo=H294GP72zhM9).\n",
        "\n",
        "The following models are available\n",
        "\n",
        "*   (Dummy Model: Always gives the correct answer to the first 7900 questions)\n",
        "*   Tiny Llama\n",
        "*   Mistral (without RAG)\n",
        "*   rag with qa embeddings\n",
        "*   Listeneintrag\n",
        "*   Listeneintrag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N6R7R18RhL1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xFRvUd8FjTnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Enter\n",
        "\n",
        "| Model | BRANCH | DIRECTORY | MODULE | CLASS | PARAMS | Note |\n",
        "|----------|----------|----------|----------|----------|----------|----------|\n",
        "|Dummy Model|main|Dummy_Model|dummy_model|dummy_model|2 test params|-|\n",
        "|Tiny Llama|tiny_llama|TinyLlama|tiny_llama|tinyllama|-|**very slow**|\n",
        "|Mistral without RAG|mistral-instruct-no-rag|MistralInstruct|mistral_instruct|mistral_instruct|-|**Huggingface Token and GPU required**|\n",
        "|RAG with QA Embeddings|rag_qa_embeddings|RAG_QA_Embeddings|rag_qa_embeddings|rag_qa_embeddings|k=contexts|-|"
      ],
      "metadata": {
        "id": "iO2nNW6kjU1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qH1mqdP9XZBY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"BRANCH\"] = \"main\"\n",
        "os.environ[\"DIRECTORY\"] = \"Dummy_Model\"\n",
        "os.environ[\"MODULE\"] = \"dummy_model\"\n",
        "os.environ[\"CLASS\"] = \"\tdummy_model\"\n",
        "os.environ[\"PARAMS\"] = \"hello world\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following script to setup the evaluation environment"
      ],
      "metadata": {
        "id": "LFeS1s_SYl3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = os.environ[\"DIRECTORY\"]\n",
        "\n",
        "!git clone --no-checkout https://github.com/dimitrod/ehu_nlp_dimathina.git\n",
        "%cd ehu_nlp_dimathina/\n",
        "!git sparse-checkout init --cone\n",
        "!git sparse-checkout set Evaluation/\n",
        "!git checkout main\n",
        "%mv Evaluation/ ..\n",
        "!git sparse-checkout set $DIRECTORY\n",
        "!git checkout $BRANCH\n",
        "%mv $DIRECTORY ..\n",
        "%cd ..\n",
        "!git clone https://github.com/mandarjoshi90/triviaqa.git\n",
        "%rm -r ehu_nlp_dimathina\n",
        "%rm -r sample_data\n",
        "%cd triviaqa/\n",
        "!pip install -r requirements.txt\n",
        "%cd ../Evaluation\n",
        "!pip install -r requirements.txt\n",
        "%cd ../{directory}\n",
        "!pip install -r requirements.txt\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBLrZP5RYNg4",
        "outputId": "93d7fc26-35ae-4c18-e662-9c15f69c1e0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ehu_nlp_dimathina'...\n",
            "remote: Enumerating objects: 417, done.\u001b[K\n",
            "remote: Counting objects: 100% (269/269), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 417 (delta 138), reused 187 (delta 94), pack-reused 148 (from 1)\u001b[K\n",
            "Receiving objects: 100% (417/417), 28.07 MiB | 22.38 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/ehu_nlp_dimathina\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
            "Switched to a new branch 'main'\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "/content\n",
            "Cloning into 'triviaqa'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Total 70 (delta 0), reused 0 (delta 0), pack-reused 70 (from 1)\u001b[K\n",
            "Receiving objects: 100% (70/70), 20.60 KiB | 811.00 KiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "/content/triviaqa\n",
            "Requirement already satisfied: tensorflow>=0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=0.11->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=0.11->-r requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=0.11->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=0.11->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=0.11->-r requirements.txt (line 1)) (0.1.2)\n",
            "/content/Evaluation\n",
            "Collecting datasets (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (4.66.6)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "/content/Dummy_Model\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CoPJQ6D3jpq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the Evaluation Data\n",
        "\n",
        "**If you don't want to change the size evaluation dataset skip this step**\n",
        "\n",
        "This step is only required if you want to test the model with a larger dataset than the first 7900 questions of the train split of the triviaqa wikipedia dataset. In case you want to test a rag model that also uses the web dataset as vector database you can increase the evaluation dataset to the required first 9500 questions of the train split following these steps"
      ],
      "metadata": {
        "id": "H294GP72zhM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m Evaluation.utils.load_evaluation_data --split_size 9500"
      ],
      "metadata": {
        "id": "cV7pwZT11Pik",
        "outputId": "90861563-87f8-4f8e-9e57-fe36d5abcccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md: 100% 26.7k/26.7k [00:00<00:00, 54.3MB/s]\n",
            "Resolving data files: 100% 26/26 [00:00<00:00, 86.16it/s]\n",
            "train-00000-of-00007.parquet: 100% 240M/240M [00:01<00:00, 161MB/s]\n",
            "train-00001-of-00007.parquet: 100% 261M/261M [00:02<00:00, 106MB/s]\n",
            "train-00002-of-00007.parquet: 100% 319M/319M [00:01<00:00, 183MB/s]\n",
            "train-00003-of-00007.parquet: 100% 266M/266M [00:04<00:00, 55.5MB/s]\n",
            "train-00004-of-00007.parquet: 100% 240M/240M [00:01<00:00, 152MB/s]\n",
            "train-00005-of-00007.parquet: 100% 259M/259M [00:02<00:00, 88.6MB/s]\n",
            "train-00006-of-00007.parquet: 100% 253M/253M [00:05<00:00, 45.7MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 235M/235M [00:03<00:00, 75.4MB/s]\n",
            "test-00000-of-00001.parquet: 100% 221M/221M [00:02<00:00, 84.0MB/s]\n",
            "Generating train split: 100% 61888/61888 [00:40<00:00, 1523.56 examples/s]\n",
            "Generating validation split: 100% 7993/7993 [00:05<00:00, 1365.50 examples/s]\n",
            "Generating test split: 100% 7701/7701 [00:03<00:00, 1942.78 examples/s]\n",
            "Loading dataset: 100% 9500/9500 [00:09<00:00, 950.41it/s] \n",
            "Replacing field names: 100% 19/19 [00:21<00:00,  1.14s/it]\n",
            "Removing Wiki Context: 100% 9500/9500 [00:00<00:00, 437021.67it/s]\n",
            "Saving file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JPZ3SoyezfjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing the Evaluation Chain\n",
        "Now you can run the Evaluation chain with the model of choice. In this case it is a dummy model to test the functionality of the evaluation chain. The dummy model always looksup the correct answer to every question.\n",
        "\n",
        "To start the evaluation chain type the following command\n",
        "\n",
        "`!python3 -m Evaluation.evaluation_chain --model_directory $DIRECTORY --model_module $MODULE --model_class $CLASS`\n",
        "\n",
        "\n",
        "\n",
        "*   If the model requires params you can set them by adding `--model_params $PARAMS`\n",
        "*   If you want to evaluate the model on a smaller split you can do that by adding --`split_size {split_size}`"
      ],
      "metadata": {
        "id": "6uaV1IoKc2Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m Evaluation.evaluation_chain --model_directory $DIRECTORY --model_module $MODULE --model_class $CLASS --model_params $PARAMS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Uk8IFdc1qA",
        "outputId": "d3e09762-3c83-4573-d8c8-927217f0e9c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rLoading Questions:   0% 0/7993 [00:00<?, ?it/s]\rLoading Questions: 100% 7993/7993 [00:00<00:00, 121867.25it/s]\n",
            "Loading model...\n",
            "This is param 1: hello\n",
            "This is param 2: world\n",
            "Model initiated\n",
            "Starting QA...\n",
            "\rRetrieving Answers:   0% 0/7993 [00:00<?, ?it/s]\rRetrieving Answers: 100% 7993/7993 [00:00<00:00, 979463.36it/s]\n",
            "QA complete\n",
            "Starting evaluation...\n",
            "Evaluation complete\n",
            "Saving results...\n",
            "Test results saved under Evaluation/results/dummy_model_split_size=7993_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the test are saved in the file *dummy_model_split_size=7900_results.txt* in the Evaluation package\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9nCQuGJNgqxV"
      }
    }
  ]
}